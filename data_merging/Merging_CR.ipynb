{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d62383-4124-48bb-b457-22db5c57be24",
   "metadata": {},
   "source": [
    "# for AdvBench dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9b01c-95c9-4801-b1ff-263b31122c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from util import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('../harmfulness/data/unlearning_data_AdvBench_3B.json', 'r') as file:\n",
    "    reject_data = json.load(file)\n",
    "with open(\"HarryPotter_sentences_sampeld.json\", \"r\") as file:\n",
    "    content = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad00eb9-62b7-4dba-9202-565944fd7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"[YOUR API KEY]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5299d79-b40d-4493-a42a-dcdee7a4daa6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prompts used\n",
    "prompt_initial_system = \"\"\"\n",
    "You are an expert text rewriter tasked with creating natural synthetic data for given requests.\n",
    "\"\"\"\n",
    "prompt_initial = \"\"\"\n",
    "Original Text: [ORIGINAL]\n",
    "\n",
    "Please create a synthetic data that maintains the semantics of the original text while naturally and briefly incorporating the specified content:\n",
    "[CONTENT].\n",
    "\n",
    "Ensure that the semantic similarity between the original text and synthetic data is high.\n",
    "Enclose your synthetic data between [DATA] an [/DATA] tokens. \n",
    "\n",
    "Here is an example:\n",
    "[DATA]\n",
    "SYNTHETIC DATA HERE\n",
    "[/DATA]\n",
    "\"\"\"\n",
    "\n",
    "prompt_evaluate_system = \"\"\"\n",
    "You are an expert text evaluator tasked with evaluating synthetic data for semantic and syntactic coherence.\n",
    "\"\"\"\n",
    "prompt_evaluate = \"\"\"\n",
    "Evaluate the synthetic data provided to determine:\n",
    "1) Its degree of semantic coherence, which assesses how logically the ideas are connected.\n",
    "2) Its degree of syntactic coherence, which evaluates how grammatically correct and structurally sound the text is.\n",
    "\n",
    "Please score the semantic and syntactic coherence of the synthetic data on a scale from 0.00 (no coherence) to 1.00 (perfect coherence).\n",
    "\n",
    "Only give the floating score; other mention or explanation is not allowed.\n",
    "\n",
    "Synthetic Data for Evaluation:\n",
    "[SYNTHETIC]\n",
    "\n",
    "Your output should follow this format:\n",
    "[SEMANTIC] YOUR SCORE ONLY\n",
    "[SYNTACTIC] YOUR SCORE ONLY\n",
    "\n",
    "Here is an example:\n",
    "[SEMANTIC] 0.55\n",
    "[SYNTACTIC] 0.55\n",
    "\"\"\"\n",
    "\n",
    "prompt_rewrite_system = \"\"\"\n",
    "You are an expert text rewriting optimizer tasked with rewriting natural synthetic data to maximize the given score.\n",
    "\"\"\"\n",
    "prompt_rewrite = \"\"\"\n",
    "Rewrite the synthetic data to maximize its overall score, favoring substitutions and reorderings over additions.\n",
    "\n",
    "Overall score S is calculated by the formula: S = 2*S_ori+S_con+0.5*(S_sem+S_syn).\n",
    "\n",
    "Where:\n",
    "- S_ori refers to the Rouge-L Recall between the original text and the synthetic text, measuring how much of the original content is captured in the synthetic text.\n",
    "- S_con refers to the Rouge-L Recall between the content incorporated into the synthetic data and the synthetic text, assessing how well the new content is integrated.\n",
    "- S_sem refers to the semantic coherence of the synthetic text, scaled from 0.00 (no coherence) to 1.00 (perfect coherence), assessing how logically the ideas are connected.\n",
    "- S_syn refers to the syntactic coherence of the synthetic text, scaled from 0.00 (no coherence) to 1.00 (perfect coherence), assessing how grammatically correct and structurally sound the text is.\n",
    "\n",
    "Here are the original text, incorporated content, past synthetic data, and their scores:\n",
    "\n",
    "Original Text: [ORIGINAL]\n",
    "\n",
    "Incorporated Content: [CONTENT]\n",
    "\n",
    "\"\"\"\n",
    "prompt_rewrite2 = \"\"\"\n",
    "Enclose your synthetic data between [DATA] and [/DATA] tokens. \n",
    "\n",
    "Here is an example:\n",
    "[DATA]\n",
    "SYNTHETIC DATA HERE\n",
    "[/DATA]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3949e-d404-4c83-9cc2-16ecedfbc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters=4\n",
    "results_dict = {i: [] for i in range(1, max_iters+1)}\n",
    "for k in range(1, max_iters+1):\n",
    "    print(f\"ITER:{k}\")\n",
    "    replies_current = rewrite_process(\n",
    "        reject_data, content, results_dict, k,\n",
    "        prompt_initial, prompt_initial_system,\n",
    "        prompt_rewrite, prompt_rewrite_system, prompt_rewrite2,\n",
    "        client\n",
    "    )\n",
    "    results_dict = eval_process(\n",
    "        replies_current, reject_data, content, results_dict, k,\n",
    "        prompt_evaluate, prompt_evaluate_system, client\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a98904-2ef3-46d0-abdc-6046b6afb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./agents/results_dict_CR.json', 'w') as file:\n",
    "    json.dump(results_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2d90f-6161-4254-bdae-de5b832c2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_score_texts = [\"\"]*len(reject_data)\n",
    "highest_scores = [0]*len(reject_data)\n",
    "for i in range(4):\n",
    "    for ind, d in enumerate(results_dict[i+1]):\n",
    "        data, s, s_ori, s_con, s_sem, s_syn = d\n",
    "        if highest_scores[ind] <= s: \n",
    "            highest_scores[ind] = s\n",
    "            highest_score_texts[ind] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ff02b-b304-41a3-8950-d16e8350f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./agents/GPT_results_score_CR.json', 'w') as file:\n",
    "    json.dump(highest_score_texts, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwku",
   "language": "python",
   "name": "rwku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
