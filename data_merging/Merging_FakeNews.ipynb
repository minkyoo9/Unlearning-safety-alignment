{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d62383-4124-48bb-b457-22db5c57be24",
   "metadata": {},
   "source": [
    "# for AdvBench dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9b01c-95c9-4801-b1ff-263b31122c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "with open('../harmfulness/data/unlearning_data_AdvBench_3B.json', 'r') as file:\n",
    "    reject_data = json.load(file)\n",
    "with open(\"FakeNews_sentences_sampeld.json\", \"r\") as file:\n",
    "    content = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad00eb9-62b7-4dba-9202-565944fd7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"[YOUR API KEY]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5299d79-b40d-4493-a42a-dcdee7a4daa6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prompts used\n",
    "prompt_initial_system = \"\"\"\n",
    "You are an expert text rewriter tasked with creating natural synthetic data for given requests.\n",
    "\"\"\"\n",
    "prompt_initial = \"\"\"\n",
    "Original Text: [ORIGINAL]\n",
    "\n",
    "Please create a synthetic data that maintains the semantics of the original text while naturally and briefly incorporating the specified content:\n",
    "[CONTENT].\n",
    "\n",
    "Ensure that the semantic similarity between the original text and synthetic data is high.\n",
    "Enclose your synthetic data between [DATA] an [/DATA] tokens. \n",
    "\n",
    "Here is an example:\n",
    "[DATA]\n",
    "SYNTHETIC DATA HERE\n",
    "[/DATA]\n",
    "\"\"\"\n",
    "\n",
    "prompt_evaluate_system = \"\"\"\n",
    "You are an expert text evaluator tasked with evaluating synthetic data for semantic and syntactic coherence.\n",
    "\"\"\"\n",
    "prompt_evaluate = \"\"\"\n",
    "Evaluate the synthetic data provided to determine:\n",
    "1) Its degree of semantic coherence, which assesses how logically the ideas are connected.\n",
    "2) Its degree of syntactic coherence, which evaluates how grammatically correct and structurally sound the text is.\n",
    "\n",
    "Please score the semantic and syntactic coherence of the synthetic data on a scale from 0.00 (no coherence) to 1.00 (perfect coherence).\n",
    "\n",
    "Only give the floating score; other mention or explanation is not allowed.\n",
    "\n",
    "Synthetic Data for Evaluation:\n",
    "[SYNTHETIC]\n",
    "\n",
    "Your output should follow this format:\n",
    "[SEMANTIC] YOUR SCORE ONLY\n",
    "[SYNTACTIC] YOUR SCORE ONLY\n",
    "\n",
    "Here is an example:\n",
    "[SEMANTIC] 0.55\n",
    "[SYNTACTIC] 0.55\n",
    "\"\"\"\n",
    "\n",
    "prompt_rewrite_system = \"\"\"\n",
    "You are an expert text rewriting optimizer tasked with rewriting natural synthetic data to maximize the given score.\n",
    "\"\"\"\n",
    "prompt_rewrite = \"\"\"\n",
    "Rewrite the synthetic data to maximize its overall score, favoring substitutions and reorderings over additions.\n",
    "\n",
    "Overall score S is calculated by the formula: S = 2*S_ori+S_con+0.5*(S_sem+S_syn).\n",
    "\n",
    "Where:\n",
    "- S_ori refers to the Rouge-L Recall between the original text and the synthetic text, measuring how much of the original content is captured in the synthetic text.\n",
    "- S_con refers to the Rouge-L Recall between the content incorporated into the synthetic data and the synthetic text, assessing how well the new content is integrated.\n",
    "- S_sem refers to the semantic coherence of the synthetic text, scaled from 0.00 (no coherence) to 1.00 (perfect coherence), assessing how logically the ideas are connected.\n",
    "- S_syn refers to the syntactic coherence of the synthetic text, scaled from 0.00 (no coherence) to 1.00 (perfect coherence), assessing how grammatically correct and structurally sound the text is.\n",
    "\n",
    "Here are the original text, incorporated content, past synthetic data, and their scores:\n",
    "\n",
    "Original Text: [ORIGINAL]\n",
    "\n",
    "Incorporated Content: [CONTENT]\n",
    "\n",
    "\"\"\"\n",
    "prompt_rewrite2 = \"\"\"\n",
    "Enclose your synthetic data between [DATA] and [/DATA] tokens. \n",
    "\n",
    "Here is an example:\n",
    "[DATA]\n",
    "SYNTHETIC DATA HERE\n",
    "[/DATA]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa97f5-f95e-4d0d-8f9b-546e97ca93dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initial_prompt(current_data, content):\n",
    "    prompts_initial = []\n",
    "    for s,c in zip(current_data, content):\n",
    "        p_ = prompt_initial\n",
    "        p_ = p_.replace('[ORIGINAL]', s['text'])\n",
    "        p_ = p_.replace('[CONTENT]', c)\n",
    "        prompts_initial.append(p_)\n",
    "    return prompts_initial\n",
    "    \n",
    "def run_initial(prompts_initial):\n",
    "    print('run initial rewriting')\n",
    "    replies_initial = []\n",
    "    for pt in tqdm(prompts_initial):\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_initial_system},\n",
    "                {\"role\": \"user\", \"content\": pt},\n",
    "            ],\n",
    "            model=\"gpt-4o-2024-05-13\",\n",
    "        )\n",
    "        replies_initial.append(chat_completion.choices[0].message.content)\n",
    "    return replies_initial\n",
    "\n",
    "def eval_prompt(initial_rewritten_data):\n",
    "    prompts_eval = []\n",
    "    for s in (initial_rewritten_data):\n",
    "        p_ = prompt_evaluate\n",
    "        p_ = p_.replace('[SYNTHETIC]', s['text'])\n",
    "        prompts_eval.append(p_)\n",
    "    return prompts_eval\n",
    "    \n",
    "def run_eval(prompts_eval):\n",
    "    print('run eval')\n",
    "    replies_eval = []\n",
    "    for pt in tqdm(prompts_eval):\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_evaluate_system},\n",
    "                {\"role\": \"user\", \"content\": pt},\n",
    "            ],\n",
    "            model=\"gpt-4o-2024-05-13\",\n",
    "        )\n",
    "        replies_eval.append(chat_completion.choices[0].message.content)\n",
    "    return replies_eval\n",
    "\n",
    "def rewrite_prompt(original, content):\n",
    "    prompts_rewrite = []\n",
    "    for o, c in zip(original, content):\n",
    "        p_ = prompt_rewrite\n",
    "        p_ = p_.replace('[ORIGINAL]', o['text'])\n",
    "        p_ = p_.replace('[CONTENT]', c)\n",
    "        prompts_rewrite.append(p_)\n",
    "    return prompts_rewrite\n",
    "    \n",
    "def run_rewrite(prompts_rewrite, results_dict, num):\n",
    "    replies_rewrite = []\n",
    "    for ind, pt in tqdm(enumerate(prompts_rewrite), total=len(prompts_rewrite)):\n",
    "        for i in range(num-1):\n",
    "            results = results_dict[i+1] # (data, s, s_ori, s_con, s_sem, s_syn)\n",
    "            data, s, s_ori, s_con, s_sem, s_syn = results[ind]\n",
    "            pt += f\"Synthetic Data_{i+1}: {data['text']}\\n S:{s}\\n S_ori:{s_ori}\\n S_con:{s_con}\\n S_sem:{s_sem}\\n S_syn:{s_syn}\\n\"\n",
    "        pt += prompt_rewrite2\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_rewrite_system},\n",
    "                {\"role\": \"user\", \"content\": pt},\n",
    "            ],\n",
    "            model=\"gpt-4o-2024-05-13\",\n",
    "        )\n",
    "        replies_rewrite.append(chat_completion.choices[0].message.content)\n",
    "    return replies_rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d28e98-c21d-4a3d-8338-4c830dacdb0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_data_content(text):\n",
    "    pattern = re.compile(r'\\[DATA\\](.*?)\\[/DATA\\]', re.DOTALL)\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return match.group(1).strip() \n",
    "    else:\n",
    "        return \"No data found\"\n",
    "def data_parsing(replies):\n",
    "    parsed_data = []\n",
    "    for d in replies:\n",
    "        parsed_data.append({'text':extract_data_content(d)})\n",
    "    return parsed_data\n",
    "        \n",
    "def calculate_similarity(list1, list2, list3):\n",
    "    texts1 = [item['text'] for item in list1]\n",
    "    texts2 = [item for item in list2]\n",
    "    texts3 = [item['text'] for item in list3]\n",
    "    \n",
    "    rouge = Rouge()\n",
    "\n",
    "    rouge_scores_list1_list2 = []\n",
    "    rouge_scores_list1_list3 = []\n",
    "\n",
    "    for text1, text2 in zip(texts1, texts2):\n",
    "        score = rouge.get_scores(text1,text2)[0]['rouge-l']['r'] # hyp, ref\n",
    "        rouge_scores_list1_list2.append(score)\n",
    "\n",
    "    for text1, text3 in zip(texts1, texts3):\n",
    "        score = rouge.get_scores(text1,text3)[0]['rouge-l']['r'] #hyp, ref\n",
    "        rouge_scores_list1_list3.append(score)\n",
    "\n",
    "    return rouge_scores_list1_list2, rouge_scores_list1_list3 # sim_con, sim_ori\n",
    "\n",
    "def extract_eval_scores(replies_eval):\n",
    "    semantic_scores, syntactic_scores = [], []\n",
    "    semantic_pattern = re.compile(r'\\[SEMANTIC\\]\\s*(\\d+\\.\\d+)')\n",
    "    syntactic_pattern = re.compile(r'\\[SYNTACTIC\\]\\s*(\\d+\\.\\d+)')\n",
    "\n",
    "    for ind, reply in enumerate(replies_eval):\n",
    "        # SEMANTIC score extraction\n",
    "        semantic_match = semantic_pattern.search(reply)\n",
    "        if semantic_match:\n",
    "            semantic_scores.append(float(semantic_match.group(1)))\n",
    "        else:\n",
    "            print(ind)\n",
    "            print(f\"SEMANTIC score not found or invalid format in: {reply}\")\n",
    "            semantic_scores.append(None)  \n",
    "\n",
    "        # SYNTACTIC score extraction\n",
    "        syntactic_match = syntactic_pattern.search(reply)\n",
    "        if syntactic_match:\n",
    "            syntactic_scores.append(float(syntactic_match.group(1)))\n",
    "        else:\n",
    "            print(ind)\n",
    "            print(f\"SYNTACTIC score not found or invalid format in: {reply}\")\n",
    "            syntactic_scores.append(None)  \n",
    "            \n",
    "    return semantic_scores, syntactic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535db7f-60bc-4450-8281-6b3c12ad69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_process(reject_data, results_dict, iter_):\n",
    "    if iter_ == 1: ## inital rewrite\n",
    "        ## Initial rewrite\n",
    "        prompts_initial = initial_prompt(reject_data, content)\n",
    "        replies_initial = run_initial(prompts_initial)\n",
    "        replies_current = data_parsing(replies_initial)\n",
    "    else: \n",
    "        prompts_rewrite = rewrite_prompt(reject_data, content)\n",
    "        replies_rewrite = run_rewrite(prompts_rewrite, results_dict, iter_)\n",
    "        replies_current = data_parsing(replies_rewrite)\n",
    "    return replies_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ea635-cd8b-4f32-8788-85e3adfa2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_process(replies, reject_data, results_dict, iter_):\n",
    "    prompts_eval = eval_prompt(replies)\n",
    "    replies_eval = run_eval(prompts_eval)\n",
    "    semantic_scores, syntactic_scores = extract_eval_scores(replies_eval)\n",
    "    sim_con, sim_ori = calculate_similarity(replies, content, reject_data)\n",
    "    for data, s_ori, s_con, s_sem, s_syn in zip(replies, sim_ori, sim_con, semantic_scores, syntactic_scores):\n",
    "        s = 2*s_ori+s_con+0.5*(s_sem+s_syn)\n",
    "        results_dict[iter_].append((data, s, s_ori, s_con, s_sem, s_syn))\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3949e-d404-4c83-9cc2-16ecedfbc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters=4\n",
    "results_dict = {1:[], 2:[], 3:[], 4:[]}\n",
    "for k in range(1, max_iters+1):\n",
    "    print(f\"ITER:{k}\")\n",
    "    replies_current = rewrite_process(reject_data, results_dict, k)\n",
    "    results_dict = eval_process(replies_current, reject_data, results_dict, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a98904-2ef3-46d0-abdc-6046b6afb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./agents/results_dict_FN.json', 'w') as file:\n",
    "    json.dump(results_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2d90f-6161-4254-bdae-de5b832c2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_score_texts = [\"\"]*len(reject_data)\n",
    "highest_scores = [0]*len(reject_data)\n",
    "for i in range(4):\n",
    "    for ind, d in enumerate(results_dict[i+1]):\n",
    "        data, s, s_ori, s_con, s_sem, s_syn = d\n",
    "        if highest_scores[ind] <= s: \n",
    "            highest_scores[ind] = s\n",
    "            highest_score_texts[ind] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ff02b-b304-41a3-8950-d16e8350f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./agents/GPT_results_score_FN.json', 'w') as file:\n",
    "    json.dump(highest_score_texts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488bc6b-28f6-45f2-8dd8-0e941ea1208e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwku",
   "language": "python",
   "name": "rwku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
