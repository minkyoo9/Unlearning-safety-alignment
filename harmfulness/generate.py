# Copyright 2023 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Evaluation of the safety of QA pairs generated by different models."""

import argparse
import json
import os

import torch
import matplotlib.pyplot as plt
import numpy as np

from datasets import load_dataset
from tqdm import tqdm

from accelerate import Accelerator

from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, default_data_collator
from datasets import Dataset
from models import load_pretrained_models
from utils import to_device
from datasets_utils import format_prompt


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--eval_dataset',
        type=str,
        required=True,
        help='Path to the input JSON file.',
    )
    parser.add_argument(
        '--num_samples',
        type=int,
        default=None,
        help='The number of samples',
    )
    parser.add_argument(
        '--model_name_or_path',
        type=str,
        required=False,
        help='Path to the model.',
    )
    parser.add_argument(
        '--expert_path',
        type=str,
        required=None,
        help='Path to the expert.',
    )
    parser.add_argument(
        '--max_length',
        type=int,
        default=512,
        help='The maximum sequence length of the model.',
    )
    parser.add_argument(
        '--batch_size',
        type=int,
        default=64,
        help='Batch size.',
    )
    parser.add_argument(
        '--torch_dtype',
        type=str,
        default='bfloat16',
        help='data type',
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        required=True,
        help='Where to store.',
    )
    parser.add_argument(
        '--icu',
        type=bool,
        default=False,
        help='In-context learning',
    )
    return parser.parse_args()

def generate_answer(prompts: list[str], model, tokenizer, max_length) -> list[str]:
    answers = []
    for prompt in tqdm(prompts):
        input_ids = to_device(
            tokenizer(prompt, return_tensors='pt'),
            device=('cuda' if torch.cuda.is_available() else None),
        )
        output_ids = model.generate(
            **input_ids,
            max_length=max_length,
        )
        answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)[len(prompt) :]
        answers.append(answer)
    return answers

def load_prompts(eval_dataset, tokenizer, num_samples=None, random_seed=42):
    prompts = []
    contexts = []
    if eval_dataset == 'HEx-PHI':
        with open(os.path.join("./data/HEx-PHI.json"), 'r') as f:
            data = json.load(f)
        for sample in data:
            prompt = format_prompt(input=sample['text'], tokenizer=tokenizer, task_prompt=False)
            prompts.append(prompt)
            contexts.append(sample['text'])
        return prompts, contexts
    elif eval_dataset == 'AdvBench':
        data = load_dataset("walledai/AdvBench")
        for sample in data['train']:
            prompt = format_prompt(input=sample['prompt'], tokenizer=tokenizer, task_prompt=False)
            prompts.append(prompt)
            contexts.append(sample['prompt'])
        return prompts, contexts
    elif eval_dataset == 'LLM-LAT_tot':
        data = load_dataset("LLM-LAT/harmful-dataset")
        for sample in data['train']:
            prompt = format_prompt(input=sample['prompt'], tokenizer=tokenizer, task_prompt=False)
            prompts.append(prompt)
            contexts.append(sample['prompt'])
        return prompts, contexts
    else:
        assert(0)

class EvalTrainer(Trainer):
    def __init__(self, max_new_tokens, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.model = self.model.to(self.accelerator.device)
        self.max_new_tokens = max_new_tokens
        
    def evaluate(
        self,
        eval_dataset
    ):
        eval_dataloader = self.get_eval_dataloader(eval_dataset)
        eval_dataloader = self.accelerator.prepare(eval_dataloader)
        
        self.model.eval()
        answer_ids = []
        for step, inputs in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):
            input_ids = inputs['input_ids'].to(self.accelerator.device)
            attention_mask = inputs['attention_mask'].to(self.accelerator.device)

            with torch.no_grad():
                outputs = self.model.generate(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            max_new_tokens=self.max_new_tokens,
                            pad_token_id=self.tokenizer.eos_token_id,
                            tokenizer=self.tokenizer,
                        )

                gathered_answer_ids = self.accelerator.gather_for_metrics(outputs, use_gather_object=True)
                if self.accelerator.is_main_process:
                    answer_ids.extend([v.cpu().numpy() for v in gathered_answer_ids])

        self.accelerator.wait_for_everyone()
        if self.accelerator.is_main_process:
            return answer_ids
        else:
            return None
    

def main() -> None:
    args = parse_arguments()
    os.makedirs(args.output_dir, exist_ok=True)

    model, tokenizer = load_pretrained_models(
            args.model_name_or_path,
            padding_side='left',
            auto_model_type=AutoModelForCausalLM,
            trust_remote_code=True,
            dtype=args.torch_dtype,
        )

    # print(model)

    # print(tokenizer.pad_token)

    prompts, contexts = load_prompts(args.eval_dataset, tokenizer, args.num_samples)

    print('Eval data example:')
    print(prompts[0])

    eval_dataset = Dataset.from_dict({'prompts': prompts})
    def tokenize_function(example):
        return tokenizer(example['prompts'], padding="max_length", truncation=True, max_length=args.max_length)
        # return tokenizer(example['prompts'], padding=True, truncation=True, max_length=args.max_length)
    tokenized_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=['prompts'])

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_eval_batch_size=args.batch_size,
    )
    
    trainer = EvalTrainer(
        model=model,
        args=training_args,
        max_new_tokens=512,
        data_collator=default_data_collator,
        tokenizer=tokenizer,
    )

    response_ids = trainer.evaluate(tokenized_dataset)

    trainer.model.cpu()
    del trainer.model 
    torch.cuda.empty_cache()

    if response_ids is not None:
        outputs = tokenizer.batch_decode(response_ids, skip_special_tokens=True)
        prompts_decoded = tokenizer.batch_decode(tokenized_dataset['input_ids'], skip_special_tokens=True)

        responses = []
        for idx, (prompt, output) in enumerate(zip(prompts_decoded, outputs, strict=True)):
            if output.startswith(prompt):
                response = output[len(prompt):]
            else:
                response = output
            responses.append(response)
    

        generation_results = []
        for context, prompt, response in zip(contexts, prompts, responses, strict=True):
            generation_results.append({'context': context, 'prompt': prompt, 'response': response})
    
        with open(f'{args.output_dir}/generation_results.json', 'w', encoding='utf-8') as f:
            json.dump(generation_results, f, ensure_ascii=False, indent=4)

        
if __name__ == '__main__':
    main()
