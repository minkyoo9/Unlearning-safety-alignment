# Copyright 2023 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Evaluation of the safety of QA pairs generated by different models."""

import argparse
import json
import os

import torch
import matplotlib.pyplot as plt
import numpy as np

from tqdm import tqdm

from transformers import AutoTokenizer, AutoModelForCausalLM

def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--results_path',
        type=str,
        required=False,
        help='Path to the generation results.',
    )
    parser.add_argument(
        '--max_length',
        type=int,
        default=512,
        help='The maximum sequence length of the model.',
    )
    return parser.parse_args()

def main() -> None:
    args = parse_arguments()

    device = 'cuda'
    
    with open(f'{args.results_path}/generation_results.json', 'r', encoding='utf-8') as f:
        generation_results = json.load(f)

    prompts = []
    responses = []
    for results in generation_results:
        prompts.append(results['prompt'])
        responses.append(results['response'])

    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-Guard-3-8B')
    model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-Guard-3-8B', torch_dtype=torch.bfloat16, device_map='auto')
    
    def moderate(chat):
        input_ids = tokenizer.apply_chat_template(chat, return_tensors="pt").to(device)
        output = model.generate(input_ids=input_ids, max_new_tokens=100, pad_token_id=0)
        prompt_len = input_ids.shape[-1]
        return tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)

    results = []
    for prompt, response in tqdm(zip(prompts, responses, strict=True)):
        output = moderate([{"role": "user", "content": prompt},
                            {"role": "assistant", "content": response}
                            ])

        from pdb import set_trace
        try:
            if 'unsafe' in output:
                is_safe, category = output.strip().split('\n')
            else:
                is_safe = output.strip()
                category = None
            # if is_safe not in ['safe', 'unsafe']:
                # set_trace()
        except:
            pass
            # set_trace()

        results.append({'prompt': prompt, 'response': response, 'output': output, 'safety': is_safe, 'category': category})
        
    
    with open(f'{args.results_path}/predictions.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)

    flagged_ratio = np.mean([pred['safety'] == 'unsafe' for pred in results])
    metrics = {'flagged_ratio': flagged_ratio}
    
    with open(f'{args.results_path}/eval_results.json', 'w', encoding='utf-8') as f:
        json.dump(metrics, f, ensure_ascii=False, indent=4)

    print('================== Result ==================')
    print(f'Flagged ratio: {flagged_ratio}')
    print('============================================')
        
if __name__ == '__main__':
    main()
